{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmaher987/Robustness---CISPA/blob/main/Results/10_PARN34_MNv2_error.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "o6iX6dDpTdq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a44603-ab39-4cf9-d712-eb5d2eb24522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "! pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the checkpoint\n",
        "ckpt_url = 'https://drive.google.com/file/d/10sHvaXhTNZGz618QmD5gSOAjO3rMzV33/view'\n",
        "torch.hub.download_url_to_file(ckpt_url, 'model_cifar_wrn.pt')"
      ],
      "metadata": {
        "id": "CqstyZ1jEWpK",
        "outputId": "7eda79cc-6683-4fc6-81ae-308992d3ae34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "81.1kB [00:00, 2.50MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "K7593B2qEgoy",
        "outputId": "3c93466b-970f-4f27-d77c-2ee682921550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  model_cifar_wrn.pt  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class attributes:\n",
        "    def __init__(self):\n",
        "        self.lr = 0.1\n",
        "        self.lr_schedule = [100, 150]\n",
        "        self.lr_factor = 0.1\n",
        "        self.epochs = 200\n",
        "        self.output = ''\n",
        "        self.model = 'MobileNetV2'\n",
        "        self.teacher_model = 'PreActResNet34'\n",
        "        self.teacher_path = \"model_cifar_wrn.pt\"\n",
        "        self.temp = 30.0\n",
        "        self.val_period = 1\n",
        "        self.save_period = 1\n",
        "        self.alpha = 0.0\n",
        "        self.dataset = 'CIFAR10'\n",
        "\n",
        "args = attributes()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "Ouhqw1XYUeOa"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr):\n",
        "    if epoch in args.lr_schedule:\n",
        "        lr *= args.lr_factor\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "if args.dataset == 'CIFAR10':\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    num_classes = 10\n",
        "elif args.dataset == 'CIFAR100':\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    num_classes = 100\n",
        "\n",
        "# num_classes = 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQIUocHMUgVb",
        "outputId": "c0515e6c-cb89-49da-9dac-64744edf25de"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth=34, num_classes=10, widen_factor=10, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "        assert ((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 1st sub-block\n",
        "        self.sub_block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "fFlWNQeniH9z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out = out + self.shortcut(x) if self.stride==1 else out\n",
        "        return out\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    #(expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 1),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  32, 3, 2),\n",
        "           (6,  64, 4, 2),\n",
        "           (6,  96, 3, 1),\n",
        "           (6, 160, 3, 2),\n",
        "           (6, 320, 1, 1)]\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(1280)\n",
        "        self.linear = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "2kdTKoN5WyOx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
        "\n",
        "def ResNet34(num_classes=10):\n",
        "    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet50(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet101(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
        "\n",
        "def ResNet152(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n"
      ],
      "metadata": {
        "id": "PZvl_WyDW2t2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Pre-activation ResNet in PyTorch.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    '''Pre-activation version of the original Bottleneck module.'''\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = self.conv3(F.relu(self.bn3(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.relu(self.bn(out))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18(num_classes=10):\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2], num_classes=num_classes)\n",
        "\n",
        "def PreActResNet34():\n",
        "    return PreActResNet(PreActBlock, [3,4,6,3])\n",
        "\n",
        "def PreActResNet50():\n",
        "    return PreActResNet(PreActBottleneck, [3,4,6,3])\n",
        "\n",
        "def PreActResNet101():\n",
        "    return PreActResNet(PreActBottleneck, [3,4,23,3])\n",
        "\n",
        "def PreActResNet152():\n",
        "    return PreActResNet(PreActBottleneck, [3,8,36,3])\n",
        "\n",
        "\n",
        "# def test():\n",
        "#     net = PreActResNet18()\n",
        "#     y = net((torch.randn(1,3,32,32)))\n",
        "#     print(y.size())\n",
        "\n",
        "# # test()"
      ],
      "metadata": {
        "id": "ONUGUelDzQEK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AttackPGD(nn.Module):\n",
        "    def __init__(self, basic_net, config):\n",
        "        super(AttackPGD, self).__init__()\n",
        "        self.basic_net = basic_net\n",
        "        self.step_size = config['step_size']\n",
        "        self.epsilon = config['epsilon']\n",
        "        self.num_steps = config['num_steps']\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        x = inputs.detach()\n",
        "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
        "        for i in range(self.num_steps):\n",
        "            x.requires_grad_()\n",
        "            with torch.enable_grad():\n",
        "                loss = F.cross_entropy(self.basic_net(x), targets, size_average=False)\n",
        "            grad = torch.autograd.grad(loss, [x])[0]\n",
        "            x = x.detach() + self.step_size * torch.sign(grad.detach())\n",
        "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
        "            x = torch.clamp(x, 0.0, 1.0)\n",
        "        return self.basic_net(x), x\n",
        "\n",
        "\n",
        "print('==> Building model..' + args.model)\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "if args.model == 'MobileNetV2':\n",
        "    basic_net = MobileNetV2()\n",
        "basic_net = basic_net.to(device)\n",
        "\n",
        "\n",
        "if args.teacher_path != '':\n",
        "    if args.teacher_model == 'PreActResNet34':\n",
        "      teacher_net = PreActResNet34()\n",
        "    teacher_net = teacher_net.to(device)\n",
        "    for param in teacher_net.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "config = {\n",
        "    'epsilon': 8.0 / 255,\n",
        "    'num_steps': 10,\n",
        "    'step_size': 2.0 / 255,\n",
        "}\n",
        "\n",
        "net = AttackPGD(basic_net, config)\n",
        "if device == 'cuda':\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "print('==> Loading teacher..')\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "def filter_state_dict(state_dict):\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    if 'state_dict' in state_dict.keys():\n",
        "        state_dict = state_dict['state_dict']\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if 'sub_block' in k:\n",
        "            continue\n",
        "        if 'module' in k:\n",
        "            new_state_dict[k[7:]] = v\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict\n",
        "\n",
        "\n",
        "ckpt = filter_state_dict(torch.load(args.teacher_path, map_location=device))\n",
        "\n",
        "teacher_net.load_state_dict(ckpt)\n",
        "teacher_net.eval()\n",
        "\n",
        "\n",
        "KL_loss = nn.KLDivLoss()\n",
        "XENT_loss = nn.CrossEntropyLoss()\n",
        "lr = args.lr"
      ],
      "metadata": {
        "id": "F_7QV7XIUjdl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "81f24a2f-9959-469e-ea9b-a7bd1e34baf4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..MobileNetV2\n",
            "==> Loading teacher..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-b65aaff54106>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mteacher_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \"functionality.\")\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '<'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(epoch, optimizer):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    iterator = tqdm(trainloader, ncols=0, leave=False)\n",
        "    for batch_idx, (inputs, targets) in enumerate(iterator):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, pert_inputs = net(inputs, targets)\n",
        "        teacher_outputs = teacher_net(inputs)\n",
        "        basic_outputs = basic_net(inputs)\n",
        "        loss = args.alpha * args.temp * args.temp * KL_loss(F.log_softmax(outputs / args.temp, dim=1),\n",
        "                                                            F.softmax(teacher_outputs / args.temp, dim=1)) + (\n",
        "                           1.0 - args.alpha) * XENT_loss(basic_outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        iterator.set_description(str(loss.item()))\n",
        "        # break -> For very slow mode\n",
        "    if (epoch + 1) % args.save_period == 0:\n",
        "        state = {\n",
        "            'net': basic_net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint/' + args.dataset + '/' + args.output + '/'):\n",
        "            os.makedirs('checkpoint/' + args.dataset + '/' + args.output + '/', )\n",
        "        torch.save(state, './checkpoint/' + args.dataset + '/' + args.output + '/epoch=' + str(epoch) + '.t7')\n",
        "    print('Mean Training Loss:', train_loss / len(iterator))\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def test(epoch, optimizer):\n",
        "    net.eval()\n",
        "    adv_correct = 0\n",
        "    natural_correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        iterator = tqdm(testloader, ncols=0, leave=False)\n",
        "        for batch_idx, (inputs, targets) in enumerate(iterator):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv_outputs, pert_inputs = net(inputs, targets)\n",
        "            natural_outputs = basic_net(inputs)\n",
        "            _, adv_predicted = adv_outputs.max(1)\n",
        "            _, natural_predicted = natural_outputs.max(1)\n",
        "            natural_correct += natural_predicted.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "            adv_correct += adv_predicted.eq(targets).sum().item()\n",
        "            iterator.set_description(str(adv_predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            # break -> For very slow mode\n",
        "    robust_acc = 100. * adv_correct / total\n",
        "    natural_acc = 100. * natural_correct / total\n",
        "    print('Natural acc:', natural_acc)\n",
        "    print('Robust acc:', robust_acc)\n",
        "    return natural_acc, robust_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "    lr = args.lr\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=2e-4)\n",
        "    for epoch in range(args.epochs):\n",
        "        print('-------------\\nEpoch ' + str(epoch+1))\n",
        "        adjust_learning_rate(optimizer, epoch, lr)\n",
        "        train_loss = train(epoch, optimizer)\n",
        "        if (epoch + 1) % args.val_period == 0:\n",
        "            natural_val, robust_val = test(epoch, optimizer)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "iCIixVGLUm1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}