{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P2mnkbczsOz"
      },
      "source": [
        "# Pre-Needs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjCAcsSZz1zX"
      },
      "source": [
        "### Wideresnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdffjykXzlfA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe-kLESTz4sD"
      },
      "source": [
        "### preactresnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egvdxU4tz6XR"
      },
      "outputs": [],
      "source": [
        "'''Pre-activation ResNet in PyTorch.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PreActBlock(nn.Module):\n",
        "    '''Pre-activation version of the BasicBlock.'''\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    '''Pre-activation version of the original Bottleneck module.'''\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(PreActBottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = self.conv3(F.relu(self.bn3(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.bn = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.relu(self.bn(out))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def PreActResNet18(num_classes=10):\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2], num_classes=num_classes)\n",
        "\n",
        "def PreActResNet34():\n",
        "    return PreActResNet(PreActBlock, [3,4,6,3])\n",
        "\n",
        "def PreActResNet50():\n",
        "    return PreActResNet(PreActBottleneck, [3,4,6,3])\n",
        "\n",
        "def PreActResNet101():\n",
        "    return PreActResNet(PreActBottleneck, [3,4,23,3])\n",
        "\n",
        "def PreActResNet152():\n",
        "    return PreActResNet(PreActBottleneck, [3,8,36,3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = PreActResNet18()\n",
        "    y = net((torch.randn(1,3,32,32)))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUq6ZaLL0B9u"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NKtd3Bv0Dx_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "################################################################\n",
        "## Components from https://github.com/davidcpage/cifar10-fast ##\n",
        "################################################################\n",
        "\n",
        "#####################\n",
        "## data preprocessing\n",
        "#####################\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465) # equals np.mean(train_set.train_data, axis=(0,1,2))/255\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616) # equals np.std(train_set.train_data, axis=(0,1,2))/255\n",
        "\n",
        "def normalise(x, mean=cifar10_mean, std=cifar10_std):\n",
        "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
        "    x -= mean*255\n",
        "    x *= 1.0/(255*std)\n",
        "    return x\n",
        "\n",
        "def pad(x, border=4):\n",
        "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
        "\n",
        "def transpose(x, source='NHWC', target='NCHW'):\n",
        "    return x.transpose([source.index(d) for d in target])\n",
        "\n",
        "#####################\n",
        "## data augmentation\n",
        "#####################\n",
        "\n",
        "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        return x[:,y0:y0+self.h,x0:x0+self.w]\n",
        "\n",
        "    def options(self, x_shape):\n",
        "        C, H, W = x_shape\n",
        "        return {'x0': range(W+1-self.w), 'y0': range(H+1-self.h)}\n",
        "\n",
        "    def output_shape(self, x_shape):\n",
        "        C, H, W = x_shape\n",
        "        return (C, self.h, self.w)\n",
        "\n",
        "class FlipLR(namedtuple('FlipLR', ())):\n",
        "    def __call__(self, x, choice):\n",
        "        return x[:, :, ::-1].copy() if choice else x\n",
        "\n",
        "    def options(self, x_shape):\n",
        "        return {'choice': [True, False]}\n",
        "\n",
        "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
        "    def __call__(self, x, x0, y0):\n",
        "        x = x.copy()\n",
        "        x[:,y0:y0+self.h,x0:x0+self.w].fill(0.0)\n",
        "        return x\n",
        "\n",
        "    def options(self, x_shape):\n",
        "        C, H, W = x_shape\n",
        "        return {'x0': range(W+1-self.w), 'y0': range(H+1-self.h)}\n",
        "\n",
        "\n",
        "class Transform():\n",
        "    def __init__(self, dataset, transforms):\n",
        "        self.dataset, self.transforms = dataset, transforms\n",
        "        self.choices = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, labels = self.dataset[index]\n",
        "        for choices, f in zip(self.choices, self.transforms):\n",
        "            args = {k: v[index] for (k,v) in choices.items()}\n",
        "            data = f(data, **args)\n",
        "        return data, labels\n",
        "\n",
        "    def set_random_choices(self):\n",
        "        self.choices = []\n",
        "        x_shape = self.dataset[0][0].shape\n",
        "        N = len(self)\n",
        "        for t in self.transforms:\n",
        "            options = t.options(x_shape)\n",
        "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape\n",
        "            self.choices.append({k:np.random.choice(v, size=N) for (k,v) in options.items()})\n",
        "\n",
        "#####################\n",
        "## dataset\n",
        "#####################\n",
        "\n",
        "def cifar10(root):\n",
        "    train_set = torchvision.datasets.CIFAR10(root=root, train=True, download=True)\n",
        "    test_set = torchvision.datasets.CIFAR10(root=root, train=False, download=True)\n",
        "    return {\n",
        "        'train': {'data': train_set.data, 'labels': train_set.targets},\n",
        "        'test': {'data': test_set.data, 'labels': test_set.targets}\n",
        "    }\n",
        "\n",
        "#####################\n",
        "## data loading\n",
        "#####################\n",
        "\n",
        "class Batches():\n",
        "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.set_random_choices = set_random_choices\n",
        "        self.dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.set_random_choices:\n",
        "            self.dataset.set_random_choices()\n",
        "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMAxA2ulzBCe"
      },
      "source": [
        "# CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa9Qb6Ar0lKk"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odWCBjA90nXW"
      },
      "outputs": [],
      "source": [
        "\n",
        "mu = torch.tensor(cifar10_mean).view(3,1,1).cuda()\n",
        "std = torch.tensor(cifar10_std).view(3,1,1).cuda()\n",
        "\n",
        "def normalize(X):\n",
        "    return (X - mu)/std\n",
        "\n",
        "upper_limit, lower_limit = 1,0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNaKmKZ20ROl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def clamp(X, lower_limit, upper_limit):\n",
        "    return torch.max(torch.min(X, upper_limit), lower_limit)\n",
        "\n",
        "\n",
        "class Batches():\n",
        "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.set_random_choices = set_random_choices\n",
        "        self.dataloader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.set_random_choices:\n",
        "            self.dataset.set_random_choices()\n",
        "        return ({'input': x.to(device).float(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).cuda()\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts,\n",
        "               norm, early_stop=False,\n",
        "               mixup=False, y_a=None, y_b=None, lam=None):\n",
        "    max_loss = torch.zeros(y.shape[0]).cuda()\n",
        "    max_delta = torch.zeros_like(X).cuda()\n",
        "    for _ in range(restarts):\n",
        "        delta = torch.zeros_like(X).cuda()\n",
        "        if norm == \"l_inf\":\n",
        "            delta.uniform_(-epsilon, epsilon)\n",
        "        elif norm == \"l_2\":\n",
        "            delta.normal_()\n",
        "            d_flat = delta.view(delta.size(0),-1)\n",
        "            n = d_flat.norm(p=2,dim=1).view(delta.size(0),1,1,1)\n",
        "            r = torch.zeros_like(n).uniform_(0, 1)\n",
        "            delta *= r/n*epsilon\n",
        "        else:\n",
        "            raise ValueError\n",
        "        delta = clamp(delta, lower_limit-X, upper_limit-X)\n",
        "        delta.requires_grad = True\n",
        "        for _ in range(attack_iters):\n",
        "            output = model(normalize(X + delta))\n",
        "            if early_stop:\n",
        "                index = torch.where(output.max(1)[1] == y)[0]\n",
        "            else:\n",
        "                index = slice(None,None,None)\n",
        "            if not isinstance(index, slice) and len(index) == 0:\n",
        "                break\n",
        "            if mixup:\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                loss = mixup_criterion(criterion, model(normalize(X+delta)), y_a, y_b, lam)\n",
        "            else:\n",
        "                loss = F.cross_entropy(output, y)\n",
        "            loss.backward()\n",
        "            grad = delta.grad.detach()\n",
        "            d = delta[index, :, :, :]\n",
        "            g = grad[index, :, :, :]\n",
        "            x = X[index, :, :, :]\n",
        "            if norm == \"l_inf\":\n",
        "                d = torch.clamp(d + alpha * torch.sign(g), min=-epsilon, max=epsilon)\n",
        "            elif norm == \"l_2\":\n",
        "                g_norm = torch.norm(g.view(g.shape[0],-1),dim=1).view(-1,1,1,1)\n",
        "                scaled_g = g/(g_norm + 1e-10)\n",
        "                d = (d + scaled_g*alpha).view(d.size(0),-1).renorm(p=2,dim=0,maxnorm=epsilon).view_as(d)\n",
        "            d = clamp(d, lower_limit - x, upper_limit - x)\n",
        "            delta.data[index, :, :, :] = d\n",
        "            delta.grad.zero_()\n",
        "        if mixup:\n",
        "            criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "            all_loss = mixup_criterion(criterion, model(normalize(X+delta)), y_a, y_b, lam)\n",
        "        else:\n",
        "            all_loss = F.cross_entropy(model(normalize(X+delta)), y, reduction='none')\n",
        "        max_delta[all_loss >= max_loss] = delta.detach()[all_loss >= max_loss]\n",
        "        max_loss = torch.max(max_loss, all_loss)\n",
        "    return max_delta\n",
        "\n",
        "\n",
        "class args:\n",
        "    def __init__(self):\n",
        "        self.model = 'PreActResNet18'\n",
        "        self.l2 = 0.0\n",
        "        self.l1 = 0.0\n",
        "        self.batch_size = 128\n",
        "        self.data_dir = '../cifar-data'\n",
        "        self.epochs = 1\n",
        "        self.lr_schedule = 'piecewise'\n",
        "        self.lr_max = 0.1\n",
        "        self.lr_one_drop = 0.01\n",
        "        self.lr_drop_epoch = 100\n",
        "        self.attack = 'pgd'\n",
        "        self.epsilon = 8\n",
        "        self.attack_iters = 10\n",
        "        self.restarts = 1\n",
        "        self.pgd_alpha = 2.0\n",
        "        self.fgsm_alpha = 1.25\n",
        "        self.norm = 'l_inf'\n",
        "        self.fgsm_init = 'random'\n",
        "        self.fname = 'cifar_model'\n",
        "        self.seed = 0\n",
        "        self.half = False\n",
        "        self.width_factor = 10\n",
        "        self.resume = 0\n",
        "        self.cutout = False\n",
        "        self.cutout_len = None\n",
        "        self.mixup = False\n",
        "        self.mixup_alpha = None\n",
        "        self.eval = False\n",
        "        self.val = False\n",
        "        self.chkpt_iters = 10\n",
        "\n",
        "def get_args():\n",
        "  return args()\n",
        "\n",
        "def get_args_expired():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model', default='PreActResNet18')\n",
        "    parser.add_argument('--l2', default=0, type=float)\n",
        "    parser.add_argument('--l1', default=0, type=float)\n",
        "    parser.add_argument('--batch-size', default=128, type=int)\n",
        "    parser.add_argument('--data-dir', default='../cifar-data', type=str)\n",
        "    parser.add_argument('--epochs', default=200, type=int)\n",
        "    parser.add_argument('--lr-schedule', default='piecewise', choices=['superconverge', 'piecewise', 'linear', 'piecewisesmoothed', 'piecewisezoom', 'onedrop', 'multipledecay', 'cosine'])\n",
        "    parser.add_argument('--lr-max', default=0.1, type=float)\n",
        "    parser.add_argument('--lr-one-drop', default=0.01, type=float)\n",
        "    parser.add_argument('--lr-drop-epoch', default=100, type=int)\n",
        "    parser.add_argument('--attack', default='pgd', type=str, choices=['pgd', 'fgsm', 'free', 'none'])\n",
        "    parser.add_argument('--epsilon', default=8, type=int)\n",
        "    parser.add_argument('--attack-iters', default=10, type=int)\n",
        "    parser.add_argument('--restarts', default=1, type=int)\n",
        "    parser.add_argument('--pgd-alpha', default=2, type=float)\n",
        "    parser.add_argument('--fgsm-alpha', default=1.25, type=float)\n",
        "    parser.add_argument('--norm', default='l_inf', type=str, choices=['l_inf', 'l_2'])\n",
        "    parser.add_argument('--fgsm-init', default='random', choices=['zero', 'random', 'previous'])\n",
        "    parser.add_argument('--fname', default='cifar_model', type=str)\n",
        "    parser.add_argument('--seed', default=0, type=int)\n",
        "    parser.add_argument('--half', action='store_true')\n",
        "    parser.add_argument('--width-factor', default=10, type=int)\n",
        "    parser.add_argument('--resume', default=0, type=int)\n",
        "    parser.add_argument('--cutout', action='store_true')\n",
        "    parser.add_argument('--cutout-len', type=int)\n",
        "    parser.add_argument('--mixup', action='store_true')\n",
        "    parser.add_argument('--mixup-alpha', type=float)\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--val', action='store_true')\n",
        "    parser.add_argument('--chkpt-iters', default=10, type=int)\n",
        "    return parser.parse_args()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en1Gx7nX0yrP",
        "outputId": "f247e4ce-3824-439f-e694-5f53ff23faa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../cifar-data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:14<00:00, 11425913.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../cifar-data/cifar-10-python.tar.gz to ../cifar-data\n",
            "Files already downloaded and verified\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Current Epoch: 0"
          ]
        }
      ],
      "source": [
        "\n",
        "# I have to rewrite the get_args function\n",
        "def main():\n",
        "    print('1')\n",
        "    args = get_args()\n",
        "    print('2')\n",
        "\n",
        "    if not os.path.exists(args.fname):\n",
        "        os.makedirs(args.fname)\n",
        "    print('3')\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logging.basicConfig(\n",
        "        format='[%(asctime)s] - %(message)s',\n",
        "        datefmt='%Y/%m/%d %H:%M:%S',\n",
        "        level=logging.DEBUG,\n",
        "        handlers=[\n",
        "            logging.FileHandler(os.path.join(args.fname, 'eval.log' if args.eval else 'output.log')),\n",
        "            logging.StreamHandler()\n",
        "        ])\n",
        "\n",
        "    logger.info(args)\n",
        "    print('4')\n",
        "\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    transforms = [Crop(32, 32), FlipLR()]\n",
        "    print('5')\n",
        "    if args.cutout:\n",
        "        transforms.append(Cutout(args.cutout_len, args.cutout_len))\n",
        "    if args.val:\n",
        "        try:\n",
        "            dataset = torch.load(\"cifar10_validation_split.pth\")\n",
        "        except:\n",
        "            print(\"Couldn't find a dataset with a validation split, did you run \"\n",
        "                  \"generate_validation.py?\")\n",
        "            return\n",
        "        val_set = list(zip(transpose(dataset['val']['data']/255.), dataset['val']['labels']))\n",
        "        val_batches = Batches(val_set, args.batch_size, shuffle=False, num_workers=2)\n",
        "    else:\n",
        "        dataset = cifar10(args.data_dir)\n",
        "    train_set = list(zip(transpose(pad(dataset['train']['data'], 4)/255.),\n",
        "        dataset['train']['labels']))\n",
        "    train_set_x = Transform(train_set, transforms)\n",
        "    train_batches = Batches(train_set_x, args.batch_size, shuffle=True, set_random_choices=True, num_workers=2)\n",
        "\n",
        "    test_set = list(zip(transpose(dataset['test']['data']/255.), dataset['test']['labels']))\n",
        "    test_batches = Batches(test_set, args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    epsilon = (args.epsilon / 255.)\n",
        "    pgd_alpha = (args.pgd_alpha / 255.)\n",
        "\n",
        "    print('6')\n",
        "\n",
        "    if args.model == 'PreActResNet18':\n",
        "        model = PreActResNet18()\n",
        "    elif args.model == 'WideResNet':\n",
        "        model = WideResNet(34, 10, widen_factor=args.width_factor, dropRate=0.0)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model\")\n",
        "\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "    model.train()\n",
        "\n",
        "    print('7')\n",
        "\n",
        "    if args.l2:\n",
        "        decay, no_decay = [], []\n",
        "        for name,param in model.named_parameters():\n",
        "            if 'bn' not in name and 'bias' not in name:\n",
        "                decay.append(param)\n",
        "            else:\n",
        "                no_decay.append(param)\n",
        "        params = [{'params':decay, 'weight_decay':args.l2},\n",
        "                  {'params':no_decay, 'weight_decay': 0 }]\n",
        "    else:\n",
        "        params = model.parameters()\n",
        "\n",
        "    opt = torch.optim.SGD(params, lr=args.lr_max, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print('8')\n",
        "\n",
        "    if args.attack == 'free':\n",
        "        delta = torch.zeros(args.batch_size, 3, 32, 32).cuda()\n",
        "        delta.requires_grad = True\n",
        "    elif args.attack == 'fgsm' and args.fgsm_init == 'previous':\n",
        "        delta = torch.zeros(args.batch_size, 3, 32, 32).cuda()\n",
        "        delta.requires_grad = True\n",
        "\n",
        "    if args.attack == 'free':\n",
        "        epochs = int(math.ceil(args.epochs / args.attack_iters))\n",
        "    else:\n",
        "        epochs = args.epochs\n",
        "\n",
        "    if args.lr_schedule == 'superconverge':\n",
        "        lr_schedule = lambda t: np.interp([t], [0, args.epochs * 2 // 5, args.epochs], [0, args.lr_max, 0])[0]\n",
        "    elif args.lr_schedule == 'piecewise':\n",
        "        def lr_schedule(t):\n",
        "            if t / args.epochs < 0.5:\n",
        "                return args.lr_max\n",
        "            elif t / args.epochs < 0.75:\n",
        "                return args.lr_max / 10.\n",
        "            else:\n",
        "                return args.lr_max / 100.\n",
        "    elif args.lr_schedule == 'linear':\n",
        "        lr_schedule = lambda t: np.interp([t], [0, args.epochs // 3, args.epochs * 2 // 3, args.epochs], [args.lr_max, args.lr_max, args.lr_max / 10, args.lr_max / 100])[0]\n",
        "    elif args.lr_schedule == 'onedrop':\n",
        "        def lr_schedule(t):\n",
        "            if t < args.lr_drop_epoch:\n",
        "                return args.lr_max\n",
        "            else:\n",
        "                return args.lr_one_drop\n",
        "    elif args.lr_schedule == 'multipledecay':\n",
        "        def lr_schedule(t):\n",
        "            return args.lr_max - (t//(args.epochs//10))*(args.lr_max/10)\n",
        "    elif args.lr_schedule == 'cosine':\n",
        "        def lr_schedule(t):\n",
        "            return args.lr_max * 0.5 * (1 + np.cos(t / args.epochs * np.pi))\n",
        "\n",
        "\n",
        "    print('9')\n",
        "\n",
        "\n",
        "    best_test_robust_acc = 0\n",
        "    best_val_robust_acc = 0\n",
        "    if args.resume:\n",
        "        start_epoch = args.resume\n",
        "        model.load_state_dict(torch.load(os.path.join(args.fname, f'model_{start_epoch-1}.pth')))\n",
        "        opt.load_state_dict(torch.load(os.path.join(args.fname, f'opt_{start_epoch-1}.pth')))\n",
        "        logger.info(f'Resuming at epoch {start_epoch}')\n",
        "\n",
        "        best_test_robust_acc = torch.load(os.path.join(args.fname, f'model_best.pth'))['test_robust_acc']\n",
        "        if args.val:\n",
        "            best_val_robust_acc = torch.load(os.path.join(args.fname, f'model_val.pth'))['val_robust_acc']\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    if args.eval:\n",
        "        if not args.resume:\n",
        "            logger.info(\"No model loaded to evaluate, specify with --resume FNAME\")\n",
        "            return\n",
        "        logger.info(\"[Evaluation mode]\")\n",
        "\n",
        "\n",
        "    print('10')\n",
        "\n",
        "    logger.info('Epoch \\t Train Time \\t Test Time \\t LR \\t \\t Train Loss \\t Train Acc \\t Train Robust Loss \\t Train Robust Acc \\t Test Loss \\t Test Acc \\t Test Robust Loss \\t Test Robust Acc')\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        print(f\"\\rCurrent Epoch: {epoch}\", end=\"\")\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        train_robust_loss = 0\n",
        "        train_robust_acc = 0\n",
        "        train_n = 0\n",
        "        for i, batch in enumerate(train_batches):\n",
        "            if args.eval:\n",
        "                break\n",
        "            X, y = batch['input'], batch['target']\n",
        "            if args.mixup:\n",
        "                X, y_a, y_b, lam = mixup_data(X, y, args.mixup_alpha)\n",
        "                X, y_a, y_b = map(Variable, (X, y_a, y_b))\n",
        "            lr = lr_schedule(epoch + (i + 1) / len(train_batches))\n",
        "            opt.param_groups[0].update(lr=lr)\n",
        "\n",
        "            if args.attack == 'pgd':\n",
        "                # Random initialization\n",
        "                if args.mixup:\n",
        "                    delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm, mixup=True, y_a=y_a, y_b=y_b, lam=lam)\n",
        "                else:\n",
        "                    delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm)\n",
        "                delta = delta.detach()\n",
        "            elif args.attack == 'fgsm':\n",
        "                delta = attack_pgd(model, X, y, epsilon, args.fgsm_alpha*epsilon, 1, 1, args.norm)\n",
        "            # Standard training\n",
        "            elif args.attack == 'none':\n",
        "                delta = torch.zeros_like(X)\n",
        "\n",
        "            robust_output = model(normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)))\n",
        "            if args.mixup:\n",
        "                robust_loss = mixup_criterion(criterion, robust_output, y_a, y_b, lam)\n",
        "            else:\n",
        "                robust_loss = criterion(robust_output, y)\n",
        "\n",
        "            if args.l1:\n",
        "                for name,param in model.named_parameters():\n",
        "                    if 'bn' not in name and 'bias' not in name:\n",
        "                        robust_loss += args.l1*param.abs().sum()\n",
        "\n",
        "            opt.zero_grad()\n",
        "            robust_loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            output = model(normalize(X))\n",
        "            if args.mixup:\n",
        "                loss = mixup_criterion(criterion, output, y_a, y_b, lam)\n",
        "            else:\n",
        "                loss = criterion(output, y)\n",
        "\n",
        "            train_robust_loss += robust_loss.item() * y.size(0)\n",
        "            train_robust_acc += (robust_output.max(1)[1] == y).sum().item()\n",
        "            train_loss += loss.item() * y.size(0)\n",
        "            train_acc += (output.max(1)[1] == y).sum().item()\n",
        "            train_n += y.size(0)\n",
        "\n",
        "        train_time = time.time()\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        test_robust_loss = 0\n",
        "        test_robust_acc = 0\n",
        "        test_n = 0\n",
        "        for i, batch in enumerate(test_batches):\n",
        "            X, y = batch['input'], batch['target']\n",
        "\n",
        "            # Random initialization\n",
        "            if args.attack == 'none':\n",
        "                delta = torch.zeros_like(X)\n",
        "            else:\n",
        "                delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm, early_stop=args.eval)\n",
        "            delta = delta.detach()\n",
        "\n",
        "            robust_output = model(normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)))\n",
        "            robust_loss = criterion(robust_output, y)\n",
        "\n",
        "            output = model(normalize(X))\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            test_robust_loss += robust_loss.item() * y.size(0)\n",
        "            test_robust_acc += (robust_output.max(1)[1] == y).sum().item()\n",
        "            test_loss += loss.item() * y.size(0)\n",
        "            test_acc += (output.max(1)[1] == y).sum().item()\n",
        "            test_n += y.size(0)\n",
        "\n",
        "        test_time = time.time()\n",
        "\n",
        "        if args.val:\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            val_robust_loss = 0\n",
        "            val_robust_acc = 0\n",
        "            val_n = 0\n",
        "            for i, batch in enumerate(val_batches):\n",
        "                X, y = batch['input'], batch['target']\n",
        "\n",
        "                # Random initialization\n",
        "                if args.attack == 'none':\n",
        "                    delta = torch.zeros_like(X)\n",
        "                else:\n",
        "                    delta = attack_pgd(model, X, y, epsilon, pgd_alpha, args.attack_iters, args.restarts, args.norm, early_stop=args.eval)\n",
        "                delta = delta.detach()\n",
        "\n",
        "                robust_output = model(normalize(torch.clamp(X + delta[:X.size(0)], min=lower_limit, max=upper_limit)))\n",
        "                robust_loss = criterion(robust_output, y)\n",
        "\n",
        "                output = model(normalize(X))\n",
        "                loss = criterion(output, y)\n",
        "\n",
        "                val_robust_loss += robust_loss.item() * y.size(0)\n",
        "                val_robust_acc += (robust_output.max(1)[1] == y).sum().item()\n",
        "                val_loss += loss.item() * y.size(0)\n",
        "                val_acc += (output.max(1)[1] == y).sum().item()\n",
        "                val_n += y.size(0)\n",
        "\n",
        "        if not args.eval:\n",
        "            logger.info('%d \\t %.1f \\t \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f \\t \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f',\n",
        "                epoch, train_time - start_time, test_time - train_time, lr,\n",
        "                train_loss/train_n, train_acc/train_n, train_robust_loss/train_n, train_robust_acc/train_n,\n",
        "                test_loss/test_n, test_acc/test_n, test_robust_loss/test_n, test_robust_acc/test_n)\n",
        "\n",
        "            if args.val:\n",
        "                logger.info('validation %.4f \\t %.4f \\t %.4f \\t %.4f',\n",
        "                    val_loss/val_n, val_acc/val_n, val_robust_loss/val_n, val_robust_acc/val_n)\n",
        "\n",
        "                if val_robust_acc/val_n > best_val_robust_acc:\n",
        "                    torch.save({\n",
        "                            'state_dict':model.state_dict(),\n",
        "                            'test_robust_acc':test_robust_acc/test_n,\n",
        "                            'test_robust_loss':test_robust_loss/test_n,\n",
        "                            'test_loss':test_loss/test_n,\n",
        "                            'test_acc':test_acc/test_n,\n",
        "                            'val_robust_acc':val_robust_acc/val_n,\n",
        "                            'val_robust_loss':val_robust_loss/val_n,\n",
        "                            'val_loss':val_loss/val_n,\n",
        "                            'val_acc':val_acc/val_n,\n",
        "                        }, os.path.join(args.fname, f'model_val.pth'))\n",
        "                    best_val_robust_acc = val_robust_acc/val_n\n",
        "\n",
        "            # save checkpoint\n",
        "            if (epoch+1) % args.chkpt_iters == 0 or epoch+1 == epochs:\n",
        "                torch.save(model.state_dict(), os.path.join(args.fname, f'model_{epoch}.pth'))\n",
        "                torch.save(opt.state_dict(), os.path.join(args.fname, f'opt_{epoch}.pth'))\n",
        "\n",
        "            # save best\n",
        "            if test_robust_acc/test_n > best_test_robust_acc:\n",
        "                torch.save({\n",
        "                        'state_dict':model.state_dict(),\n",
        "                        'test_robust_acc':test_robust_acc/test_n,\n",
        "                        'test_robust_loss':test_robust_loss/test_n,\n",
        "                        'test_loss':test_loss/test_n,\n",
        "                        'test_acc':test_acc/test_n,\n",
        "                    }, os.path.join(args.fname, f'model_best.pth'))\n",
        "                best_test_robust_acc = test_robust_acc/test_n\n",
        "        else:\n",
        "            logger.info('%d \\t %.1f \\t \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f \\t \\t %.4f \\t %.4f \\t %.4f \\t \\t %.4f',\n",
        "                epoch, train_time - start_time, test_time - train_time, -1,\n",
        "                -1, -1, -1, -1,\n",
        "                test_loss/test_n, test_acc/test_n, test_robust_loss/test_n, test_robust_acc/test_n)\n",
        "            return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ff_MWyzEXi"
      },
      "source": [
        "# CIFAR-100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VDK7m8EzF6L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY-TL8NIzGBy"
      },
      "source": [
        "# SVHN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pK_0c_nzKrq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}