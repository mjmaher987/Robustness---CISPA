{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5zenEjP5w4lfO5eynD7K+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjmaher987/Robustness---CISPA/blob/main/Codes/4-Adversarially_Robust_Distillation-Without_PTH_epoch_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o6iX6dDpTdq6"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# sys.argv = ['ipykernel_launcher.py', '–lr', '0.1', '–epochs', '20', '–model', 'MobileNetV2', '–teacher_model', 'ResNet18', '–teacher_path', 'what??!', '–dataset','CIFAR10']\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--lr_schedule', type=int, nargs='+', default=[100, 150],\n",
        "#                     help='Decrease learning rate at these epochs.')\n",
        "# parser.add_argument('--lr_factor', default=0.1, type=float, help='factor by which to decrease lr')\n",
        "# parser.add_argument('--epochs', default=20, type=int, help='number of epochs for training')\n",
        "# parser.add_argument('--output', default='', type=str, help='output subdirectory')\n",
        "# parser.add_argument('--model', default='MobileNetV2', type=str, help='student model name')\n",
        "# parser.add_argument('--teacher_model', default='ResNet18', type=str, help='teacher network model')\n",
        "# parser.add_argument('--teacher_path', default='what??!', type=str, help='path of teacher net being distilled')\n",
        "# parser.add_argument('--temp', default=30.0, type=float, help='temperature for distillation')\n",
        "# parser.add_argument('--val_period', default=1, type=int, help='print every __ epoch')\n",
        "# parser.add_argument('--save_period', default=1, type=int, help='save every __ epoch')\n",
        "# parser.add_argument('--alpha', default=1.0, type=float, help='weight for sum of losses')\n",
        "# parser.add_argument('--dataset', default='CIFAR10', type=str, help='name of dataset')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "class attributes:\n",
        "    def __init__(self):\n",
        "        self.lr = 0.1\n",
        "        self.lr_schedule = [100, 150]\n",
        "        self.lr_factor = 0.1\n",
        "        self.epochs = 5\n",
        "        self.output = ''\n",
        "        self.model = 'MobileNetV2'\n",
        "        self.teacher_model = 'ResNet18'\n",
        "        self.teacher_path = 'what??!'\n",
        "        self.temp = 30.0\n",
        "        self.val_period = 1\n",
        "        self.save_period = 1\n",
        "        self.alpha = 1.0\n",
        "        self.dataset = 'CIFAR10'\n",
        "\n",
        "args = attributes()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "Ouhqw1XYUeOa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr):\n",
        "    if epoch in args.lr_schedule:\n",
        "        lr *= args.lr_factor\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "if args.dataset == 'CIFAR10':\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    num_classes = 10\n",
        "elif args.dataset == 'CIFAR100':\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    num_classes = 100\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQIUocHMUgVb",
        "outputId": "86f202e1-bb30-4f91-af11-d3b55067c2a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:14<00:00, 11997717.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out = out + self.shortcut(x) if self.stride==1 else out\n",
        "        return out\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    #(expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 1),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  32, 3, 2),\n",
        "           (6,  64, 4, 2),\n",
        "           (6,  96, 3, 1),\n",
        "           (6, 160, 3, 2),\n",
        "           (6, 320, 1, 1)]\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(1280)\n",
        "        self.linear = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "2kdTKoN5WyOx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
        "\n",
        "def ResNet34(num_classes=10):\n",
        "    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet50(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet101(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
        "\n",
        "def ResNet152(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n"
      ],
      "metadata": {
        "id": "PZvl_WyDW2t2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AttackPGD(nn.Module):\n",
        "    def __init__(self, basic_net, config):\n",
        "        super(AttackPGD, self).__init__()\n",
        "        self.basic_net = basic_net\n",
        "        self.step_size = config['step_size']\n",
        "        self.epsilon = config['epsilon']\n",
        "        self.num_steps = config['num_steps']\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        x = inputs.detach()\n",
        "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
        "        for i in range(self.num_steps):\n",
        "            x.requires_grad_()\n",
        "            with torch.enable_grad():\n",
        "                loss = F.cross_entropy(self.basic_net(x), targets, size_average=False)\n",
        "            grad = torch.autograd.grad(loss, [x])[0]\n",
        "            x = x.detach() + self.step_size * torch.sign(grad.detach())\n",
        "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
        "            x = torch.clamp(x, 0.0, 1.0)\n",
        "        return self.basic_net(x), x\n",
        "\n",
        "\n",
        "print('==> Building model..' + args.model)\n",
        "if args.model == 'MobileNetV2':\n",
        "    basic_net = MobileNetV2(num_classes=num_classes)\n",
        "elif args.model == 'WideResNet':\n",
        "    basic_net = WideResNet(num_classes=num_classes)\n",
        "elif args.model == 'ResNet18':\n",
        "    basic_net = ResNet18(num_classes=num_classes)\n",
        "basic_net = basic_net.to(device)\n",
        "if args.teacher_path != '':\n",
        "    if args.teacher_model == 'MobileNetV2':\n",
        "        teacher_net = MobileNetV2(num_classes=num_classes)\n",
        "    elif args.teacher_model == 'WideResNet':\n",
        "        teacher_net = WideResNet(num_classes=num_classes)\n",
        "    elif args.teacher_model == 'ResNet18':\n",
        "        teacher_net = ResNet18(num_classes=num_classes)\n",
        "    teacher_net = teacher_net.to(device)\n",
        "    for param in teacher_net.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "config = {\n",
        "    'epsilon': 8.0 / 255,\n",
        "    'num_steps': 10,\n",
        "    'step_size': 2.0 / 255,\n",
        "}\n",
        "net = AttackPGD(basic_net, config)\n",
        "if device == 'cuda':\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "print('==> Loading teacher..')\n",
        "# teacher_net.load_state_dict(torch.load(args.teacher_path))\n",
        "# teacher_net.eval()\n",
        "\n",
        "KL_loss = nn.KLDivLoss()\n",
        "XENT_loss = nn.CrossEntropyLoss()\n",
        "lr = args.lr\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_7QV7XIUjdl",
        "outputId": "2a2b2dc0-0b47-4e0d-f016-149c5175ed8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..MobileNetV2\n",
            "==> Loading teacher..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(epoch, optimizer):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    iterator = tqdm(trainloader, ncols=0, leave=False)\n",
        "    for batch_idx, (inputs, targets) in enumerate(iterator):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, pert_inputs = net(inputs, targets)\n",
        "        teacher_outputs = teacher_net(inputs)\n",
        "        basic_outputs = basic_net(inputs)\n",
        "        loss = args.alpha * args.temp * args.temp * KL_loss(F.log_softmax(outputs / args.temp, dim=1),\n",
        "                                                            F.softmax(teacher_outputs / args.temp, dim=1)) + (\n",
        "                           1.0 - args.alpha) * XENT_loss(basic_outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        iterator.set_description(str(loss.item()))\n",
        "        # break\n",
        "    if (epoch + 1) % args.save_period == 0:\n",
        "        state = {\n",
        "            'net': basic_net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint/' + args.dataset + '/' + args.output + '/'):\n",
        "            os.makedirs('checkpoint/' + args.dataset + '/' + args.output + '/', )\n",
        "        torch.save(state, './checkpoint/' + args.dataset + '/' + args.output + '/epoch=' + str(epoch) + '.t7')\n",
        "    print('Mean Training Loss:', train_loss / len(iterator))\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def test(epoch, optimizer):\n",
        "    net.eval()\n",
        "    adv_correct = 0\n",
        "    natural_correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        iterator = tqdm(testloader, ncols=0, leave=False)\n",
        "        for batch_idx, (inputs, targets) in enumerate(iterator):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv_outputs, pert_inputs = net(inputs, targets)\n",
        "            natural_outputs = basic_net(inputs)\n",
        "            _, adv_predicted = adv_outputs.max(1)\n",
        "            _, natural_predicted = natural_outputs.max(1)\n",
        "            natural_correct += natural_predicted.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "            adv_correct += adv_predicted.eq(targets).sum().item()\n",
        "            iterator.set_description(str(adv_predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            # break\n",
        "    robust_acc = 100. * adv_correct / total\n",
        "    natural_acc = 100. * natural_correct / total\n",
        "    print('Natural acc:', natural_acc)\n",
        "    print('Robust acc:', robust_acc)\n",
        "    return natural_acc, robust_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "    lr = args.lr\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=2e-4)\n",
        "    for epoch in range(args.epochs):\n",
        "        adjust_learning_rate(optimizer, epoch, lr)\n",
        "        train_loss = train(epoch, optimizer)\n",
        "        if (epoch + 1) % args.val_period == 0:\n",
        "            natural_val, robust_val = test(epoch, optimizer)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCIixVGLUm1x",
        "outputId": "b5e639e2-06ad-4a7b-f94f-67253d87f391"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.001591254682804141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 10.0\n",
            "Robust acc: 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.001136958597155283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 10.0\n",
            "Robust acc: 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.0010255126534458583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 10.0\n",
            "Robust acc: 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.0009703184266591354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 10.0\n",
            "Robust acc: 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.000937488585677179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 10.0\n",
            "Robust acc: 9.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}