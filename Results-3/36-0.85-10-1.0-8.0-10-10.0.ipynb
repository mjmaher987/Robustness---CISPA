{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o6iX6dDpTdq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62adb25-339c-49c1-fe4f-e372eb09764b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "! pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the checkpoint\n",
        "# ckpt_url = 'https://drive.google.com/file/d/10sHvaXhTNZGz618QmD5gSOAjO3rMzV33/view'\n",
        "# torch.hub.download_url_to_file(ckpt_url, 'model_cifar_wrn.pt')\n",
        "# torch.hub.download_url_to_file('https://drive.google.com/uc?id=10sHvaXhTNZGz618QmD5gSOAjO3rMzV33', 'model_cifar_wrn.pt')\n",
        "import gdown\n",
        "gdown.download(f'https://drive.google.com/uc?id=10sHvaXhTNZGz618QmD5gSOAjO3rMzV33', 'model_cifar_wrn.pt', quiet=False)"
      ],
      "metadata": {
        "id": "CqstyZ1jEWpK",
        "outputId": "b0a29074-ee67-4cbd-876c-c1f9ebcd7c30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10sHvaXhTNZGz618QmD5gSOAjO3rMzV33\n",
            "To: /content/model_cifar_wrn.pt\n",
            "100%|██████████| 193M/193M [00:00<00:00, 288MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_cifar_wrn.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "K7593B2qEgoy",
        "outputId": "5f8cd6eb-16af-4653-c7ed-dab2cd1c1d53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/  \u001b[01;34mdata\u001b[0m/  model_cifar_wrn.pt  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class attributes:\n",
        "    def __init__(self):\n",
        "        self.lr = 0.85\n",
        "        self.lr_schedule = [100, 150]\n",
        "        self.lr_factor = 0.85\n",
        "        self.epochs = 10\n",
        "        self.output = ''\n",
        "        self.model = 'ResNet18'\n",
        "        self.teacher_model = 'WideResNet34'\n",
        "        self.teacher_path = \"model_cifar_wrn.pt\"\n",
        "        self.temp = 30.0\n",
        "        self.val_period = 1\n",
        "        self.save_period = 1\n",
        "        self.alpha = 1.0\n",
        "        self.dataset = 'CIFAR10'\n",
        "\n",
        "args = attributes()\n",
        "\n",
        "config = {\n",
        "    'epsilon': 8.0 / 255,\n",
        "    'num_steps': 10,\n",
        "    'step_size': 10.0 / 255,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Ouhqw1XYUeOa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, lr):\n",
        "    if epoch in args.lr_schedule:\n",
        "        lr *= args.lr_factor\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "if args.dataset == 'CIFAR10':\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    num_classes = 10\n",
        "elif args.dataset == 'CIFAR100':\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "    num_classes = 100\n",
        "\n",
        "# num_classes = 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQIUocHMUgVb",
        "outputId": "a6aa010b-dd7e-4514-a82e-f0ab1d2ee08c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out = out + self.shortcut(x) if self.stride==1 else out\n",
        "        return out\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    #(expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 1),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  32, 3, 2),\n",
        "           (6,  64, 4, 2),\n",
        "           (6,  96, 3, 1),\n",
        "           (6, 160, 3, 2),\n",
        "           (6, 320, 1, 1)]\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(1280)\n",
        "        self.linear = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "2kdTKoN5WyOx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
        "\n",
        "def ResNet34(num_classes=10):\n",
        "    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet50(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
        "\n",
        "def ResNet101(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
        "\n",
        "def ResNet152(num_classes=10):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n",
        "\n",
        "if args.model == 'ResNet18':\n",
        "    basic_net = ResNet18(num_classes=num_classes)"
      ],
      "metadata": {
        "id": "PZvl_WyDW2t2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth=34, num_classes=10, widen_factor=10, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "        assert ((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 1st sub-block\n",
        "        self.sub_block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "fFlWNQeniH9z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AttackPGD(nn.Module):\n",
        "    def __init__(self, basic_net, config):\n",
        "        super(AttackPGD, self).__init__()\n",
        "        self.basic_net = basic_net\n",
        "        self.step_size = config['step_size']\n",
        "        self.epsilon = config['epsilon']\n",
        "        self.num_steps = config['num_steps']\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        x = inputs.detach()\n",
        "        x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
        "        for i in range(self.num_steps):\n",
        "            x.requires_grad_()\n",
        "            with torch.enable_grad():\n",
        "                loss = F.cross_entropy(self.basic_net(x), targets, size_average=False)\n",
        "            grad = torch.autograd.grad(loss, [x])[0]\n",
        "            x = x.detach() + self.step_size * torch.sign(grad.detach())\n",
        "            x = torch.min(torch.max(x, inputs - self.epsilon), inputs + self.epsilon)\n",
        "            x = torch.clamp(x, 0.0, 1.0)\n",
        "        return self.basic_net(x), x\n",
        "\n",
        "\n",
        "print('==> Building model..' + args.model)\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "basic_net = basic_net.to(device)\n",
        "\n",
        "\n",
        "if args.teacher_path != '':\n",
        "    if args.teacher_model == 'WideResNet34':\n",
        "      teacher_net = WideResNet(34, 10, widen_factor=10, dropRate=0.0)\n",
        "    teacher_net = teacher_net.to(device)\n",
        "    for param in teacher_net.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "net = AttackPGD(basic_net, config)\n",
        "if device == 'cuda':\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "print('==> Loading teacher..')\n",
        "ckpt = torch.load(args.teacher_path, map_location=device)\n",
        "\n",
        "\n",
        "teacher_net.load_state_dict(ckpt)\n",
        "teacher_net.eval()\n",
        "\n",
        "\n",
        "KL_loss = nn.KLDivLoss()\n",
        "XENT_loss = nn.CrossEntropyLoss()\n",
        "lr = args.lr"
      ],
      "metadata": {
        "id": "F_7QV7XIUjdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0d2a0d-f347-45c9-d3ad-21b9b4a3586b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..ResNet18\n",
            "==> Loading teacher..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(epoch, optimizer):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    iterator = tqdm(trainloader, ncols=0, leave=False)\n",
        "    for batch_idx, (inputs, targets) in enumerate(iterator):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, pert_inputs = net(inputs, targets)\n",
        "        teacher_outputs = teacher_net(inputs)\n",
        "        basic_outputs = basic_net(inputs)\n",
        "        loss = args.alpha * args.temp * args.temp * KL_loss(F.log_softmax(outputs / args.temp, dim=1),\n",
        "                                                            F.softmax(teacher_outputs / args.temp, dim=1)) + (\n",
        "                           1.0 - args.alpha) * XENT_loss(basic_outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        iterator.set_description(str(loss.item()))\n",
        "        # break -> For very slow mode\n",
        "    if (epoch + 1) % args.save_period == 0:\n",
        "        state = {\n",
        "            'net': basic_net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint/' + args.dataset + '/' + args.output + '/'):\n",
        "            os.makedirs('checkpoint/' + args.dataset + '/' + args.output + '/', )\n",
        "        torch.save(state, './checkpoint/' + args.dataset + '/' + args.output + '/epoch=' + str(epoch) + '.t7')\n",
        "    print('Mean Training Loss:', train_loss / len(iterator))\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def test(epoch, optimizer):\n",
        "    net.eval()\n",
        "    adv_correct = 0\n",
        "    natural_correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        iterator = tqdm(testloader, ncols=0, leave=False)\n",
        "        for batch_idx, (inputs, targets) in enumerate(iterator):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            adv_outputs, pert_inputs = net(inputs, targets)\n",
        "            natural_outputs = basic_net(inputs)\n",
        "            _, adv_predicted = adv_outputs.max(1)\n",
        "            _, natural_predicted = natural_outputs.max(1)\n",
        "            natural_correct += natural_predicted.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "            adv_correct += adv_predicted.eq(targets).sum().item()\n",
        "            iterator.set_description(str(adv_predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            # break -> For very slow mode\n",
        "    robust_acc = 100. * adv_correct / total\n",
        "    natural_acc = 100. * natural_correct / total\n",
        "    print('Natural acc:', natural_acc)\n",
        "    print('Robust acc:', robust_acc)\n",
        "    return natural_acc, robust_acc\n",
        "\n",
        "\n",
        "def main():\n",
        "    lr = args.lr\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=2e-4)\n",
        "    for epoch in range(args.epochs):\n",
        "        print('-------------\\nEpoch ' + str(epoch+1))\n",
        "        adjust_learning_rate(optimizer, epoch, lr)\n",
        "        train_loss = train(epoch, optimizer)\n",
        "        if (epoch + 1) % args.val_period == 0:\n",
        "            natural_val, robust_val = test(epoch, optimizer)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "iCIixVGLUm1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ecde45-9628-4dc7-be37-2c31a498d4b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0% 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.14616089714381397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 41.47\n",
            "Robust acc: 23.39\n",
            "-------------\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.10441850746988945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 48.07\n",
            "Robust acc: 26.7\n",
            "-------------\n",
            "Epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.08849725193913331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 55.48\n",
            "Robust acc: 28.9\n",
            "-------------\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.08048406626333666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 58.47\n",
            "Robust acc: 31.84\n",
            "-------------\n",
            "Epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.07672420038324793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 58.7\n",
            "Robust acc: 33.58\n",
            "-------------\n",
            "Epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.07293105094938937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 64.98\n",
            "Robust acc: 32.81\n",
            "-------------\n",
            "Epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.07183142469438446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 60.48\n",
            "Robust acc: 33.17\n",
            "-------------\n",
            "Epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.06960524608145284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 62.64\n",
            "Robust acc: 34.92\n",
            "-------------\n",
            "Epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.06908549335034912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 65.04\n",
            "Robust acc: 35.14\n",
            "-------------\n",
            "Epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Training Loss: 0.0683449061916155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural acc: 61.31\n",
            "Robust acc: 35.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}